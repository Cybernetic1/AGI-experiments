# GPU Server Setup Instructions

## Quick Start

### 1. Transfer Files to GPU Server

```bash
# On your local machine, tar the project
cd /home/yky/misc-programs
tar -czf AGI-experiments.tar.gz AGI-experiments/

# Transfer to GPU server (replace with your server details)
scp AGI-experiments.tar.gz user@gpu-server:~/

# On GPU server, extract
ssh user@gpu-server
cd ~
tar -xzf AGI-experiments.tar.gz
cd AGI-experiments/
```

### 2. Run Setup Script

```bash
# Make executable (if not already)
chmod +x setup_gpu_server.sh

# Run setup
./setup_gpu_server.sh
```

This will:
- Create virtual environment
- Install PyTorch with CUDA
- Install all dependencies
- Download spaCy model
- Test all core modules
- Verify GPU is available

### 3. Start Training

```bash
# Activate environment
source venv/bin/activate

# Quick test (1000 stories, ~5 minutes)
python train_symmetric.py --num_stories 1000 --epochs 10 --batch_size 32

# Full training (10K stories, ~1 hour)
python train_symmetric.py --num_stories 10000 --epochs 50 --batch_size 64

# Large-scale training (100K stories, several hours)
python train_symmetric.py --num_stories 100000 --epochs 100 --batch_size 128
```

### 4. Monitor Training

```bash
# In a separate terminal/tmux session
tensorboard --logdir runs/

# Then open browser to http://localhost:6006
```

### 5. Evaluate

```bash
# After training completes
python evaluate_symmetric.py \
    --checkpoint checkpoints/best_model.pt \
    --num_stories 100 \
    --batch_size 32
```

---

## Training Parameters

### Quick Reference

| Parameter | Default | Description |
|-----------|---------|-------------|
| `--num_stories` | 1000 | Number of stories to train on |
| `--max_seq_len` | 20 | Maximum sentence length |
| `--hidden_dim` | 128 | Hidden dimension size |
| `--num_rules` | 16 | Number of logic rules |
| `--prop_length` | 5 | Max propositions per sentence |
| `--epochs` | 50 | Number of training epochs |
| `--batch_size` | 32 | Batch size |
| `--lr` | 0.001 | Learning rate |
| `--lambda_cycle` | 0.5 | Cycle consistency weight |

### Recommended Configurations

**Small-scale test (CPU-friendly):**
```bash
python train_symmetric.py \
    --num_stories 1000 \
    --epochs 10 \
    --batch_size 16 \
    --hidden_dim 64 \
    --num_rules 8
```

**Medium-scale (single GPU):**
```bash
python train_symmetric.py \
    --num_stories 10000 \
    --epochs 50 \
    --batch_size 64 \
    --hidden_dim 128 \
    --num_rules 16
```

**Large-scale (powerful GPU):**
```bash
python train_symmetric.py \
    --num_stories 100000 \
    --epochs 100 \
    --batch_size 128 \
    --hidden_dim 256 \
    --num_rules 32
```

---

## File Structure

### Core Architecture
- `implicit_graph_wm.py` - Implicit graph working memory
- `reversible_logic_rules.py` - Bidirectional logic rules
- `symmetric_logic_network.py` - Main architecture

### Training & Evaluation
- `train_symmetric.py` - Training script
- `evaluate_symmetric.py` - Evaluation script

### Documentation
- `ARCHITECTURE_INSIGHTS.md` - Design rationale
- `PROTOTYPE_STATUS.md` - Implementation status
- `PROGRESS_UPDATE.md` - Progress tracker

### Existing Code (can be integrated later)
- `learnable_parsing.py` - NL parsing (99.3% accuracy baseline)
- `hierarchical_logic_network.py` - AR+RL dual head
- `vector_quantization.py` - VQ-VAE for propositions
- `entity_registry.py` - Entity management

---

## Expected Results

### After 10 epochs (1K stories):
- Parsing accuracy: ~60-70%
- Generation perplexity: ~100-200
- Cycle consistency: ~50-60%

### After 50 epochs (10K stories):
- Parsing accuracy: ~80-90%
- Generation perplexity: ~50-100
- Cycle consistency: ~70-80%

### After 100 epochs (100K stories):
- Parsing accuracy: ~90-95%
- Generation perplexity: ~20-50
- Cycle consistency: ~85-90%

---

## Outputs

Training creates:
```
checkpoints/
â”œâ”€â”€ best_model.pt          # Best model based on validation accuracy
â””â”€â”€ training_results.json  # Training metrics per epoch

data/
â””â”€â”€ tinystories_cache/     # Cached processed data
    â””â”€â”€ processed_10000.pt

results/
â””â”€â”€ evaluation_results.json # Detailed evaluation metrics
```

---

## Troubleshooting

### CUDA Out of Memory

Reduce batch size:
```bash
python train_symmetric.py --batch_size 16
```

Or reduce model size:
```bash
python train_symmetric.py --hidden_dim 64 --num_rules 8
```

### Slow Training

Enable dataloader workers:
- Already set to `num_workers=4` in code
- If still slow, check GPU utilization: `nvidia-smi`

### spaCy Download Fails

Manual download:
```bash
source venv/bin/activate
python -m spacy download en_core_web_sm --user
```

### Import Errors

Make sure you're in the project directory and venv is activated:
```bash
cd ~/AGI-experiments
source venv/bin/activate
python -c "import symmetric_logic_network; print('âœ“ OK')"
```

---

## Advanced Usage

### Resume Training

Training automatically saves checkpoints. To resume:
```python
# TODO: Add resume capability
# For now, training starts fresh each time
```

### Custom Dataset

Modify `TinyStoriesLogicDataset` in `train_symmetric.py` to load your own data.

### Multi-GPU Training

```python
# TODO: Add DistributedDataParallel support
# For now, uses single GPU
```

### Experiment Tracking

Training logs can be viewed with tensorboard:
```bash
tensorboard --logdir runs/ --port 6006
```

---

## Performance Benchmarks

On NVIDIA A100 (80GB):
- **Throughput**: ~1000 sentences/second
- **Memory**: ~4GB for 10K dataset
- **Training time**: ~1 hour for 50 epochs on 10K stories

On NVIDIA RTX 3090 (24GB):
- **Throughput**: ~600 sentences/second
- **Memory**: ~4GB for 10K dataset
- **Training time**: ~2 hours for 50 epochs on 10K stories

On CPU (16 cores):
- **Throughput**: ~50 sentences/second
- **Memory**: ~2GB for 10K dataset
- **Training time**: ~20 hours for 50 epochs on 10K stories

---

## Next Steps After Training

1. **Analyze Results**
   ```bash
   python evaluate_symmetric.py --checkpoint checkpoints/best_model.pt
   ```

2. **Test Multi-hop Reasoning**
   - Evaluation script includes basic tests
   - Can extend with custom test cases

3. **Integrate with VQ-VAE**
   - Add discrete proposition codes
   - Connect with `vector_quantization.py`

4. **Scale Up**
   - Train on 100K+ stories
   - Increase model capacity
   - Add more logic rules

5. **Real-world Applications**
   - Question answering (bAbI)
   - Story comprehension
   - Dialogue systems

---

## Contact

For questions or issues, see main project README.

**Good luck with training! ðŸš€**
